{"category_type":"topic","category_name":"model-serving","repos_data":[{"full_name":"vllm-project/vllm","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","topics":["gpt","llm","pytorch","llmops","mlops","model-serving","transformer","llm-serving","inference","llama"],"created_at":"2023-02-09T11:23:20Z","pushed_at":"2024-05-02T04:29:55Z","stargazers_count":18791,"language":"Python"},{"full_name":"bentoml/BentoML","description":"The most flexible way to serve AI/ML models in production - Build Model Inference Service, LLM APIs, Inference Graph/Pipelines, Compound AI systems, Multi-Modal, RAG as a Service, and more!","topics":["model-serving","model-deployment","model-management","ml-platform","ai","machine-learning","mlops","bentoml","kubernetes","deep-learning"],"created_at":"2019-04-02T01:39:27Z","pushed_at":"2024-04-30T22:54:27Z","stargazers_count":6558,"language":"Python"},{"full_name":"tensorchord/envd","description":"üèïÔ∏è Reproducible development environment","topics":["developer-tools","development-environment","docker","buildkit","hacktoberfest","llmops","mlops","mlops-workflow","model-serving"],"created_at":"2022-04-11T09:04:19Z","pushed_at":"2024-04-29T02:42:55Z","stargazers_count":1919,"language":"Makefile"}],"frecuent_topics":{"mlops":3,"model-serving":3,"llmops":2,"gpt":1,"llm":1}}