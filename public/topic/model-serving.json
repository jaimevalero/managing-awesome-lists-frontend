{"category_type":"topic","category_name":"model-serving","repos_data":[{"full_name":"vllm-project/vllm","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","topics":["gpt","llm","pytorch","model-serving","transformer","llm-serving","inference","llama","amd","cuda"],"created_at":"2023-02-09T11:23:20Z","pushed_at":"2025-12-19T16:08:53Z","stargazers_count":65779,"language":"Python"},{"full_name":"bentoml/BentoML","description":"The easiest way to serve AI apps and models - Build Model Inference APIs, Job queues, LLM apps, Multi-model pipelines, and more!","topics":["model-serving","mlops","llmops","generative-ai","llm-inference","model-inference-service","inference-platform","deep-learning","llm-serving","machine-learning"],"created_at":"2019-04-02T01:39:27Z","pushed_at":"2026-01-12T07:22:02Z","stargazers_count":8357,"language":"Python"},{"full_name":"tensorchord/envd","description":"üèïÔ∏è Reproducible development environment for humans and agents","topics":["developer-tools","development-environment","docker","buildkit","hacktoberfest","llmops","mlops","mlops-workflow","model-serving","agent"],"created_at":"2022-04-11T09:04:19Z","pushed_at":"2025-12-01T04:05:08Z","stargazers_count":2166,"language":"Makefile"}],"frecuent_topics":{"model-serving":3,"llm-serving":2,"mlops":2,"llmops":2,"gpt":1}}