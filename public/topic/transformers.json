{"category_type":"topic","category_name":"transformers","repos_data":[{"full_name":"labmlai/annotated_deep_learning_paper_implementations","description":"üßë‚Äçüè´ 60+ Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†","topics":["deep-learning","deep-learning-tutorial","pytorch","gan","transformers","reinforcement-learning","optimizers","neural-networks","transformer","machine-learning"],"created_at":"2020-08-25T02:29:34Z","pushed_at":"2025-11-11T09:22:41Z","stargazers_count":65272,"language":"Python"},{"full_name":"deepset-ai/haystack","description":"AI orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.","topics":["nlp","question-answering","pytorch","semantic-search","information-retrieval","summarization","transformers","machine-learning","ai","python"],"created_at":"2019-11-14T09:05:28Z","pushed_at":"2025-12-19T15:39:51Z","stargazers_count":23670,"language":"Python"},{"full_name":"NVIDIA/Megatron-LM","description":"Ongoing research training transformer models at scale","topics":["large-language-models","model-para","transformers"],"created_at":"2019-03-21T16:15:52Z","pushed_at":"2025-12-19T15:36:02Z","stargazers_count":14645,"language":"Python"},{"full_name":"neuml/txtai","description":"üí° All-in-one AI framework for semantic search, LLM orchestration and language model workflows","topics":["python","search","nlp","semantic-search","vector-search","txtai","llm","vector-database","language-model","transformers"],"created_at":"2020-08-09T19:14:59Z","pushed_at":"2025-12-15T18:35:56Z","stargazers_count":11946,"language":"Python"},{"full_name":"huggingface/tokenizers","description":"üí• Fast State-of-the-Art Tokenizers optimized for Research and Production","topics":["nlp","natural-language-processing","natural-language-understanding","language-model","transformers","bert","gpt"],"created_at":"2019-11-01T17:52:20Z","pushed_at":"2025-12-16T10:52:44Z","stargazers_count":10321,"language":"Rust"},{"full_name":"OpenRLHF/OpenRLHF","description":"An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO & GRPO & REINFORCE++ & TIS & vLLM & Ray & Dynamic Sampling & Async Agentic RL)","topics":["transformers","vllm","large-language-models","raylib","reinforcement-learning-from-human-feedback","reinforcement-learning","openai-o1","proximal-policy-optimization"],"created_at":"2023-07-30T02:20:13Z","pushed_at":"2025-12-18T14:18:48Z","stargazers_count":8623,"language":"Python"},{"full_name":"intel-analytics/ipex-llm","description":"Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, DeepSeek, Mixtral, Gemma, Phi, MiniCPM, Qwen-VL, MiniCPM-V, etc.) on Intel XPU (e.g., local PC with iGPU and NPU, discrete GPU such as Arc, Flex and Max); seamlessly integrate with llama.cpp, Ollama, HuggingFace, LangChain, LlamaIndex, vLLM, DeepSpeed, Axolotl, etc.","topics":["pytorch","llm","transformers","gpu"],"created_at":"2016-08-29T07:59:50Z","pushed_at":"2025-10-14T06:04:12Z","stargazers_count":8547,"language":"Shell"},{"full_name":"EleutherAI/gpt-neox","description":"An implementation of model parallel autoregressive transformers on GPUs, based on the Megatron and DeepSpeed libraries","topics":["deepspeed-library","gpt-3","transformers","language-model"],"created_at":"2020-12-22T14:37:54Z","pushed_at":"2025-12-10T21:14:29Z","stargazers_count":7353,"language":"Python"},{"full_name":"superduper-io/superduper","description":"Superduper: End-to-end framework for building custom AI applications and agents.","topics":["ai","mlops","torch","transformers","mongodb","python","pytorch","ml","database","data"],"created_at":"2022-08-30T12:51:24Z","pushed_at":"2025-09-01T15:20:18Z","stargazers_count":5246,"language":"Python"},{"full_name":"lucidrains/reformer-pytorch","description":"Reformer, the efficient Transformer, in Pytorch","topics":["artificial-intelligence","transformers","attention-mechanism","machine-learning","pytorch"],"created_at":"2020-01-09T20:42:37Z","pushed_at":"2023-06-21T14:17:49Z","stargazers_count":2191,"language":"Python"},{"full_name":"changyeyu/LLM-RL-Visualized","description":"üåü100+ ÂéüÂàõ LLM / RL ÂéüÁêÜÂõæüìöÔºå„ÄäÂ§ßÊ®°ÂûãÁÆóÊ≥ï„Äã‰ΩúËÄÖÂ∑®ÁåÆÔºÅüí•Ôºà100+  LLM/RL Algorithm Maps Ôºâ","topics":["llm","reinforcement-learning","algorithm","nlp-machine-learning","vlm","ai","deep-learning","machine-learning","natural-language-processing","transformers"],"created_at":"2025-04-26T14:30:47Z","pushed_at":"2025-12-16T11:25:22Z","stargazers_count":2021,"language":"Python"},{"full_name":"jsalt18-sentence-repl/jiant","description":"jiant is an nlp toolkit","topics":["nlp","sentence-representation","bert","multitask-learning","transformers","transfer-learning"],"created_at":"2018-06-18T18:12:47Z","pushed_at":"2023-07-06T22:00:38Z","stargazers_count":1674,"language":"Python"},{"full_name":"PyTorchLightning/lightning-transformers","description":"Flexible components pairing ü§ó Transformers with :zap: Pytorch Lightning","topics":["pytorch","pytorch-lightning","transformers","hydra"],"created_at":"2020-12-14T10:45:57Z","pushed_at":"2022-11-21T16:35:36Z","stargazers_count":613,"language":"Python"},{"full_name":"nlpodyssey/cybertron","description":"Cybertron: the home planet of the Transformers in Go","topics":["bart","bert","machine-translation","question-answering","zero-shot-classification","bert-as-service","transformers","huggingface","text-classification","named-entity-recognition"],"created_at":"2022-06-21T13:45:54Z","pushed_at":"2024-06-08T19:22:45Z","stargazers_count":321,"language":"Go"},{"full_name":"Furyton/awesome-language-model-analysis","description":"This paper list focuses on the theoretical and empirical analysis of language models, especially large language models (LLMs). The papers in this list investigate the learning behavior, generalization ability, and other properties of language models through theoretical analysis, empirical analysis, or a combination of both.","topics":["analytics","awesome","llm","transformers","ai","analysis","chatgpt","deep-learning","generative-ai","large-language-models"],"created_at":"2024-04-25T14:55:57Z","pushed_at":"2024-12-02T13:59:10Z","stargazers_count":96,"language":"Python"}],"frecuent_topics":{"transformers":15,"pytorch":6,"machine-learning":4,"nlp":4,"ai":4}}