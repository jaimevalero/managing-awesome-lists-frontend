{"category_type":"topic","category_name":"transformers","repos_data":[{"full_name":"labmlai/annotated_deep_learning_paper_implementations","description":"üßë‚Äçüè´ 60 Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†","topics":["deep-learning","deep-learning-tutorial","pytorch","gan","transformers","reinforcement-learning","optimizers","neural-networks","transformer","machine-learning"],"created_at":"2020-08-25T02:29:34Z","pushed_at":"2024-03-23T23:39:46Z","stargazers_count":48435,"language":"Python"},{"full_name":"deepset-ai/haystack","description":":mag: LLM orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.","topics":["nlp","question-answering","bert","language-model","pytorch","semantic-search","squad","information-retrieval","summarization","transformers"],"created_at":"2019-11-14T09:05:28Z","pushed_at":"2024-05-02T02:59:31Z","stargazers_count":13702,"language":"Python"},{"full_name":"NVIDIA/Megatron-LM","description":"Ongoing research training transformer models at scale","topics":["large-language-models","model-para","transformers"],"created_at":"2019-03-21T16:15:52Z","pushed_at":"2024-05-01T12:31:27Z","stargazers_count":8643,"language":"Python"},{"full_name":"huggingface/tokenizers","description":"üí• Fast State-of-the-Art Tokenizers optimized for Research and Production","topics":["nlp","natural-language-processing","natural-language-understanding","language-model","transformers","bert","gpt"],"created_at":"2019-11-01T17:52:20Z","pushed_at":"2024-04-30T13:53:47Z","stargazers_count":8448,"language":"Rust"},{"full_name":"EleutherAI/gpt-neo","description":"An implementation of model parallel GPT-2 and GPT-3-style models using the mesh-tensorflow library.","topics":["language-model","transformers","gpt","gpt-2","gpt-3"],"created_at":"2020-07-05T10:23:46Z","pushed_at":"2022-02-25T06:27:12Z","stargazers_count":8145,"language":"Python"},{"full_name":"EleutherAI/gpt-neox","description":"An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.","topics":["deepspeed-library","gpt-3","transformers","language-model"],"created_at":"2020-12-22T14:37:54Z","pushed_at":"2024-04-26T19:32:10Z","stargazers_count":6588,"language":"Python"},{"full_name":"intel-analytics/ipex-llm","description":"Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, Baichuan, Mixtral, Gemma, etc.) on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max). A PyTorch LLM library that seamlessly integrates with llama.cpp, Ollama, HuggingFace, LangChain, LlamaIndex, DeepSpeed, vLLM, FastChat, etc.","topics":["pytorch","llm","transformers","gpu"],"created_at":"2016-08-29T07:59:50Z","pushed_at":"2024-05-02T00:45:00Z","stargazers_count":5979,"language":"Shell"},{"full_name":"lucidrains/reformer-pytorch","description":"Reformer, the efficient Transformer, in Pytorch","topics":["artificial-intelligence","transformers","attention-mechanism","machine-learning","pytorch"],"created_at":"2020-01-09T20:42:37Z","pushed_at":"2023-06-21T14:17:49Z","stargazers_count":2054,"language":"Python"},{"full_name":"jsalt18-sentence-repl/jiant","description":"jiant is an nlp toolkit","topics":["nlp","sentence-representation","bert","multitask-learning","transformers","transfer-learning"],"created_at":"2018-06-18T18:12:47Z","pushed_at":"2023-07-06T22:00:38Z","stargazers_count":1608,"language":"Python"},{"full_name":"PyTorchLightning/lightning-transformers","description":"Flexible components pairing ü§ó Transformers with :zap: Pytorch Lightning","topics":["pytorch","pytorch-lightning","transformers","hydra"],"created_at":"2020-12-14T10:45:57Z","pushed_at":"2022-11-21T16:35:36Z","stargazers_count":606,"language":"Python"},{"full_name":"declare-lab/flan-alpaca","description":"This repository contains code for extending the Stanford Alpaca synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.","topics":["flan-t5","alpaca","language-model","llm","transformers"],"created_at":"2023-03-22T11:35:19Z","pushed_at":"2023-07-04T23:49:23Z","stargazers_count":336,"language":"Python"},{"full_name":"pleisto/flappy","description":"Production-Ready LLM Agent SDK for Every Developer","topics":["agent","chatgpt","generative-ai","llm","rewoo","llama","transformers"],"created_at":"2023-09-15T18:09:15Z","pushed_at":"2024-04-19T20:02:28Z","stargazers_count":304,"language":"Rust"},{"full_name":"nlpodyssey/cybertron","description":"Cybertron: the home planet of the Transformers in Go","topics":["bart","bert","machine-translation","question-answering","zero-shot-classification","bert-as-service","transformers","huggingface","text-classification","named-entity-recognition"],"created_at":"2022-06-21T13:45:54Z","pushed_at":"2024-01-10T21:57:15Z","stargazers_count":258,"language":"Go"}],"frecuent_topics":{"transformers":13,"pytorch":5,"language-model":5,"bert":4,"nlp":3}}