{"category_type":"topic","category_name":"llama3","repos_data":[{"full_name":"jmorganca/ollama","description":"Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.","topics":["llama","llm","llama2","llms","go","golang","ollama","mistral","gemma","llama3"],"created_at":"2023-06-26T19:39:32Z","pushed_at":"2024-09-13T04:27:07Z","stargazers_count":89005,"language":"TypeScript"},{"full_name":"ollama/ollama","description":"Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.","topics":["llama","llm","llama2","llms","go","golang","ollama","mistral","gemma","llama3"],"created_at":"2023-06-26T19:39:32Z","pushed_at":"2024-09-13T04:27:07Z","stargazers_count":89005,"language":"TypeScript"},{"full_name":"unslothai/unsloth","description":"Finetune Llama 3.1, Mistral, Phi & Gemma LLMs 2-5x faster with 80% less memory","topics":["ai","finetuning","fine-tuning","llama","llms","lora","mistral","qlora","gemma","llama3"],"created_at":"2023-11-29T16:50:09Z","pushed_at":"2024-09-09T02:55:52Z","stargazers_count":15508,"language":"Python"},{"full_name":"afc163/fanyi","description":"A ðŸ‡¨ðŸ‡³ and ðŸ‡ºðŸ‡¸ translator in your command line","topics":["translation","command-line","command-line-tools","nodejs","chinese","groq","llama3","translator"],"created_at":"2013-12-19T11:09:46Z","pushed_at":"2024-09-13T04:35:00Z","stargazers_count":1362,"language":"JavaScript"},{"full_name":"yusufcanb/tlm","description":"Local CLI Copilot, powered by CodeLLaMa. ðŸ’»ðŸ¦™","topics":["llm","codellama","bash","powershell","llama3","zsh"],"created_at":"2024-02-14T12:00:13Z","pushed_at":"2024-07-13T22:36:01Z","stargazers_count":1186,"language":"Dockerfile"}],"frecuent_topics":{"llama3":5,"llama":3,"llm":3,"llms":3,"mistral":3}}