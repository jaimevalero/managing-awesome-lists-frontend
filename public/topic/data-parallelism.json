{"category_type":"topic","category_name":"data-parallelism","repos_data":[{"full_name":"hpcaitech/ColossalAI","description":"Making large AI models cheaper, faster and more accessible","topics":["deep-learning","hpc","large-scale","data-parallelism","pipeline-parallelism","model-parallelism","ai","big-model","distributed-computing","inference"],"created_at":"2021-10-28T16:19:44Z","pushed_at":"2024-08-16T03:49:24Z","stargazers_count":38486,"language":"Python"},{"full_name":"microsoft/DeepSpeed","description":"DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.","topics":["deep-learning","pytorch","gpu","machine-learning","billion-parameters","data-parallelism","model-parallelism","inference","pipeline-parallelism","compression"],"created_at":"2020-01-23T18:35:18Z","pushed_at":"2024-08-16T03:59:33Z","stargazers_count":34325,"language":"Python"}],"frecuent_topics":{"deep-learning":2,"data-parallelism":2,"pipeline-parallelism":2,"model-parallelism":2,"inference":2}}