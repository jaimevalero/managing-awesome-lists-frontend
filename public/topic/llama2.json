{"category_type":"topic","category_name":"llama2","repos_data":[{"full_name":"jmorganca/ollama","description":"Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.","topics":["llama","llm","llama2","llms","go","golang","ollama","mistral","gemma","llama3"],"created_at":"2023-06-26T19:39:32Z","pushed_at":"2024-09-13T04:27:07Z","stargazers_count":89005,"language":"TypeScript"},{"full_name":"ollama/ollama","description":"Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.","topics":["llama","llm","llama2","llms","go","golang","ollama","mistral","gemma","llama3"],"created_at":"2023-06-26T19:39:32Z","pushed_at":"2024-09-13T04:27:07Z","stargazers_count":89005,"language":"TypeScript"},{"full_name":"InternLM/lmdeploy","description":"LMDeploy is a toolkit for compressing, deploying, and serving LLMs.","topics":["cuda-kernels","deepspeed","fastertransformer","llm-inference","turbomind","internlm","llama","llm","codellama","llama2"],"created_at":"2023-06-15T12:38:06Z","pushed_at":"2024-09-13T03:12:32Z","stargazers_count":4159,"language":"Python"},{"full_name":"higgsfield-ai/higgsfield","description":"Fault-tolerant, highly scalable GPU orchestration, and a machine learning framework designed for training models with billions to trillions of parameters","topics":["cluster-management","deep-learning","distributed","llama","llama2","llm","machine-learning","mlops","pytorch"],"created_at":"2018-05-26T22:47:43Z","pushed_at":"2024-05-25T17:43:07Z","stargazers_count":3289,"language":"Jupyter Notebook"},{"full_name":"Dicklesworthstone/swiss_army_llama","description":"A FastAPI service for semantic text search using precomputed embeddings and advanced similarity measures, with built-in support for various file types through textract.","topics":["embedding-similarity","embedding-vectors","embeddings","llama2","llamacpp","semantic-search"],"created_at":"2023-08-08T01:23:30Z","pushed_at":"2024-09-11T16:38:29Z","stargazers_count":919,"language":"Python"}],"frecuent_topics":{"llama2":5,"llama":4,"llm":4,"llms":2,"go":2}}