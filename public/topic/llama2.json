{"category_type":"topic","category_name":"llama2","repos_data":[{"full_name":"jmorganca/ollama","description":"Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.","topics":["llama","llm","llama2","llms","go","golang","ollama","mistral","gemma","llama3"],"created_at":"2023-06-26T19:39:32Z","pushed_at":"2024-08-16T04:07:25Z","stargazers_count":85112,"language":"TypeScript"},{"full_name":"ollama/ollama","description":"Get up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.","topics":["llama","llm","llama2","llms","go","golang","ollama","mistral","gemma","llama3"],"created_at":"2023-06-26T19:39:32Z","pushed_at":"2024-08-16T04:07:25Z","stargazers_count":85112,"language":"TypeScript"},{"full_name":"InternLM/lmdeploy","description":"LMDeploy is a toolkit for compressing, deploying, and serving LLMs.","topics":["cuda-kernels","deepspeed","fastertransformer","llm-inference","turbomind","internlm","llama","llm","codellama","llama2"],"created_at":"2023-06-15T12:38:06Z","pushed_at":"2024-08-16T04:50:37Z","stargazers_count":3882,"language":"Python"},{"full_name":"higgsfield-ai/higgsfield","description":"Fault-tolerant, highly scalable GPU orchestration, and a machine learning framework designed for training models with billions to trillions of parameters","topics":["cluster-management","deep-learning","distributed","llama","llama2","llm","machine-learning","mlops","pytorch"],"created_at":"2018-05-26T22:47:43Z","pushed_at":"2024-05-25T17:43:07Z","stargazers_count":3281,"language":"Jupyter Notebook"},{"full_name":"Dicklesworthstone/swiss_army_llama","description":"A FastAPI service for semantic text search using precomputed embeddings and advanced similarity measures, with built-in support for various file types through textract.","topics":["embedding-similarity","embedding-vectors","embeddings","llama2","llamacpp","semantic-search"],"created_at":"2023-08-08T01:23:30Z","pushed_at":"2024-08-01T17:41:34Z","stargazers_count":910,"language":"Python"}],"frecuent_topics":{"llama2":5,"llama":4,"llm":4,"llms":2,"go":2}}