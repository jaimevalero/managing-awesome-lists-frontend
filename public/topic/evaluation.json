{"category_type":"topic","category_name":"evaluation","repos_data":[{"full_name":"explodinggradients/ragas","description":"Supercharge Your LLM Application Evaluations ðŸš€","topics":["llm","llmops","evaluation"],"created_at":"2023-05-08T17:48:04Z","pushed_at":"2025-12-19T06:38:31Z","stargazers_count":11792,"language":"Python"},{"full_name":"typpo/promptfoo","description":"Test your prompts, agents, and RAGs. AI Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.","topics":["llm","prompt-engineering","prompts","llmops","prompt-testing","testing","rag","evaluation","evaluation-framework","llm-eval"],"created_at":"2023-04-28T15:48:49Z","pushed_at":"2025-12-19T16:01:54Z","stargazers_count":9484,"language":"TypeScript"},{"full_name":"Marker-Inc-Korea/AutoRAG","description":"AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation","topics":["analysis","automl","benchmarking","document-parser","embeddings","evaluation","llm","llm-evaluation","llm-ops","open-source"],"created_at":"2024-01-10T12:25:00Z","pushed_at":"2025-11-20T17:57:18Z","stargazers_count":4486,"language":"Python"},{"full_name":"langwatch/langwatch","description":"The open LLM Ops platform - Traces, Analytics, Evaluations, Datasets and Prompt Optimization âœ¨","topics":["ai","analytics","datasets","evaluation","gpt","llm","observability","openai","prompt-engineering","dspy"],"created_at":"2023-09-09T11:33:18Z","pushed_at":"2025-12-19T13:51:58Z","stargazers_count":2694,"language":"JavaScript"},{"full_name":"huggingface/lighteval","description":"Lighteval is your all-in-one toolkit for evaluating LLMs across multiple backends","topics":["evaluation","evaluation-framework","evaluation-metrics","huggingface"],"created_at":"2024-01-26T13:15:39Z","pushed_at":"2025-12-15T11:16:46Z","stargazers_count":2207,"language":"Python"},{"full_name":"PaesslerAG/gval","description":"Expression evaluation in golang","topics":["evaluate-expressions","expression-evaluator","gval","expression-language","parsing","godoc","evaluation","golang","parser","go"],"created_at":"2017-09-27T08:32:49Z","pushed_at":"2025-08-04T14:21:23Z","stargazers_count":809,"language":"Go"},{"full_name":"Psycoy/MixEval","description":"The official evaluation suite and dynamic data release for MixEval.","topics":["benchmark","benchmark-mixture","benchmarking-framework","benchmarking-suite","evaluation","evaluation-framework","foundation-models","large-language-model","large-language-models","large-multimodal-models"],"created_at":"2024-06-01T10:50:50Z","pushed_at":"2024-11-10T02:23:50Z","stargazers_count":253,"language":"Python"},{"full_name":"nullne/evaluator","description":"","topics":["golang","s-expressions","expression","evaluation","evaluator"],"created_at":"2017-04-27T18:31:46Z","pushed_at":"2023-04-14T11:05:55Z","stargazers_count":42,"language":"Go"}],"frecuent_topics":{"evaluation":8,"llm":4,"evaluation-framework":3,"llmops":2,"prompt-engineering":2}}