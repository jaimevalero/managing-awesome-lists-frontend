{"category_type":"topic","category_name":"evaluation","repos_data":[{"full_name":"typpo/promptfoo","description":"Test your prompts, agents, and RAGs. AI Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.","topics":["llm","prompt-engineering","prompts","llmops","prompt-testing","testing","rag","evaluation","evaluation-framework","llm-eval"],"created_at":"2023-04-28T15:48:49Z","pushed_at":"2026-01-19T07:05:35Z","stargazers_count":9985,"language":"TypeScript"},{"full_name":"promptfoo/promptfoo","description":"Test your prompts, agents, and RAGs. AI Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.","topics":["llm","prompt-engineering","prompts","llmops","prompt-testing","testing","rag","evaluation","evaluation-framework","llm-eval"],"created_at":"2023-04-28T15:48:49Z","pushed_at":"2026-01-19T07:05:35Z","stargazers_count":9985,"language":"TypeScript"},{"full_name":"Marker-Inc-Korea/AutoRAG","description":"AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation","topics":["analysis","automl","benchmarking","document-parser","embeddings","evaluation","llm","llm-evaluation","llm-ops","open-source"],"created_at":"2024-01-10T12:25:00Z","pushed_at":"2025-12-23T19:00:38Z","stargazers_count":4532,"language":"Python"},{"full_name":"langwatch/langwatch","description":"The open LLM Ops platform - Traces, Analytics, Evaluations, Datasets and Prompt Optimization âœ¨","topics":["ai","analytics","datasets","evaluation","gpt","llm","observability","openai","prompt-engineering","dspy"],"created_at":"2023-09-09T11:33:18Z","pushed_at":"2026-01-18T20:53:26Z","stargazers_count":2733,"language":"JavaScript"},{"full_name":"huggingface/lighteval","description":"Lighteval is your all-in-one toolkit for evaluating LLMs across multiple backends","topics":["evaluation","evaluation-framework","evaluation-metrics","huggingface"],"created_at":"2024-01-26T13:15:39Z","pushed_at":"2026-01-14T15:43:21Z","stargazers_count":2273,"language":"Python"},{"full_name":"PaesslerAG/gval","description":"Expression evaluation in golang","topics":["evaluate-expressions","expression-evaluator","gval","expression-language","parsing","godoc","evaluation","golang","parser","go"],"created_at":"2017-09-27T08:32:49Z","pushed_at":"2025-08-04T14:21:23Z","stargazers_count":810,"language":"Go"},{"full_name":"nullne/evaluator","description":"","topics":["golang","s-expressions","expression","evaluation","evaluator"],"created_at":"2017-04-27T18:31:46Z","pushed_at":"2023-04-14T11:05:55Z","stargazers_count":42,"language":"Go"}],"frecuent_topics":{"evaluation":7,"llm":4,"prompt-engineering":3,"evaluation-framework":3,"prompts":2}}