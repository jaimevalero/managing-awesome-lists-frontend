{"category_type":"topic","category_name":"flash-attention","repos_data":[{"full_name":"InternLM/InternLM","description":"Official release of InternLM2 7B and 20B base and chat models. 200K context support","topics":["chatbot","gpt","large-language-model","long-context","rlhf","fine-tuning-llm","llm","chinese","flash-attention","pretrained-models"],"created_at":"2023-07-06T04:52:06Z","pushed_at":"2024-04-22T09:03:47Z","stargazers_count":5213,"language":"Python"},{"full_name":"DefTruth/Awesome-LLM-Inference","description":"ðŸ“–A curated list of Awesome LLM Inference Paper with codes, TensorRT-LLM, vLLM, streaming-llm, AWQ, SmoothQuant, WINT8/4, Continuous Batching, FlashAttention, PagedAttention etc.","topics":["flash-attention","flash-attention-2","paged-attention","streaming-llm","streamingllm","flash-decoding","tensorrt-llm","vllm","awq","awesome-llm"],"created_at":"2023-08-27T02:32:15Z","pushed_at":"2024-05-01T06:39:19Z","stargazers_count":1330,"language":"unknown"}],"frecuent_topics":{"flash-attention":2,"chatbot":1,"gpt":1,"large-language-model":1,"long-context":1}}