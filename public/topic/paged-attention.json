{"category_type":"topic","category_name":"paged-attention","repos_data":[{"full_name":"DefTruth/Awesome-LLM-Inference","description":"ðŸ“–A curated list of Awesome LLM Inference Paper with codes, TensorRT-LLM, vLLM, streaming-llm, AWQ, SmoothQuant, WINT8/4, Continuous Batching, FlashAttention, PagedAttention etc.","topics":["flash-attention","flash-attention-2","paged-attention","tensorrt-llm","vllm","awesome-llm","sora","llm","llm-inference","llms"],"created_at":"2023-08-27T02:32:15Z","pushed_at":"2024-09-09T01:25:29Z","stargazers_count":2444,"language":"unknown"}],"frecuent_topics":{"flash-attention":1,"flash-attention-2":1,"paged-attention":1,"tensorrt-llm":1,"vllm":1}}