{"category_type":"topic","category_name":"transformer","repos_data":[{"full_name":"huggingface/transformers","description":"ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ","topics":["nlp","natural-language-processing","pytorch","pytorch-transformers","transformer","model-hub","pretrained-models","speech-recognition","hacktoberfest","python"],"created_at":"2018-10-29T13:56:00Z","pushed_at":"2026-01-19T08:42:34Z","stargazers_count":155386,"language":"Python"},{"full_name":"labmlai/annotated_deep_learning_paper_implementations","description":"üßë‚Äçüè´ 60+ Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†","topics":["deep-learning","deep-learning-tutorial","pytorch","gan","transformers","reinforcement-learning","optimizers","neural-networks","transformer","machine-learning"],"created_at":"2020-08-25T02:29:34Z","pushed_at":"2025-11-11T09:22:41Z","stargazers_count":65402,"language":"Python"},{"full_name":"EleutherAI/lm-evaluation-harness","description":"A framework for few-shot evaluation of language models.","topics":["evaluation-framework","language-model","transformer"],"created_at":"2020-08-28T00:09:15Z","pushed_at":"2026-01-16T16:38:14Z","stargazers_count":11227,"language":"Python"},{"full_name":"open-mmlab/mmsegmentation","description":"OpenMMLab Semantic Segmentation Toolbox and Benchmark.","topics":["semantic-segmentation","pytorch","pspnet","deeplabv3","transformer","swin-transformer","realtime-segmentation","vessel-segmentation","retinal-vessel-segmentation","image-segmentation"],"created_at":"2020-06-14T04:32:33Z","pushed_at":"2024-08-13T08:53:34Z","stargazers_count":9568,"language":"Python"},{"full_name":"google/trax","description":"Trax ‚Äî Deep Learning with Clear Code and Speed","topics":["jax","numpy","deep-learning","deep-reinforcement-learning","machine-learning","transformer","reinforcement-learning"],"created_at":"2019-10-05T15:09:14Z","pushed_at":"2025-09-26T14:37:32Z","stargazers_count":8301,"language":"Python"},{"full_name":"codertimo/BERT-pytorch","description":"Google AI 2018 BERT pytorch implementation","topics":["bert","transformer","pytorch","nlp","language-model"],"created_at":"2018-10-15T12:58:15Z","pushed_at":"2023-09-15T12:57:08Z","stargazers_count":6516,"language":"Python"},{"full_name":"guillaume-be/rust-bert","description":"Rust native ready-to-use NLP pipelines and transformer-based models (BERT, DistilBERT, GPT2,...)","topics":["deep-learning","nlp","transformer","bert","rust-lang","rust","machine-learning","ner","sentiment-analysis","question-answering"],"created_at":"2020-01-25T09:40:07Z","pushed_at":"2026-01-13T19:54:39Z","stargazers_count":3023,"language":"Rust"},{"full_name":"huggingface/pytorch-openai-transformer-lm","description":"üê•A PyTorch implementation of OpenAI's finetuned transformer language model with a script to import the weights pre-trained by OpenAI","topics":["neural-networks","pytorch","openai","language-model","transformer"],"created_at":"2018-06-13T14:02:41Z","pushed_at":"2021-08-09T16:17:12Z","stargazers_count":1524,"language":"Python"},{"full_name":"lemonhu/NER-BERT-pytorch","description":"PyTorch solution of named entity recognition task Using Google AI's pre-trained BERT model.","topics":["ner","named-entity-recognition","entity-extraction","chinese-ner","google-bert","transformer","msra","information-extraction","pytorch"],"created_at":"2019-01-04T08:13:41Z","pushed_at":"2023-03-30T13:57:17Z","stargazers_count":450,"language":"Python"},{"full_name":"barissayil/SentimentAnalysis","description":"Sentiment analysis neural network trained by fine-tuning BERT, ALBERT, or DistilBERT on the Stanford Sentiment Treebank.","topics":["bert","nlp","machine-learning","pytorch","pytorch-implementation","vuejs","flask","transformer","huggingface","huggingface-transformer"],"created_at":"2019-12-27T11:54:06Z","pushed_at":"2023-06-12T21:34:19Z","stargazers_count":380,"language":"Python"},{"full_name":"andresinaka/Transformer","description":"Easy Attributed String Creator","topics":["nsattributedstring","online-editor","swift","objective-c","transformer"],"created_at":"2017-10-24T14:34:38Z","pushed_at":"2020-05-18T13:53:03Z","stargazers_count":280,"language":"JavaScript"},{"full_name":"L0SG/relational-rnn-pytorch","description":"An implementation of DeepMind's Relational Recurrent Neural Networks (NeurIPS 2018) in PyTorch.","topics":["pytorch","language-model","word-language-model","language-modeling","deep-learning","recurrent-neural-networks","deepmind","transformer","self-attention"],"created_at":"2018-08-21T07:57:41Z","pushed_at":"2018-12-27T05:38:23Z","stargazers_count":247,"language":"Python"},{"full_name":"leviswind/pytorch-transformer","description":"pytorch implementation of Attention is all you need","topics":["pytorch","attention-is-all-you-need","translation","transformer"],"created_at":"2018-01-05T08:00:01Z","pushed_at":"2021-06-16T06:35:54Z","stargazers_count":239,"language":"Python"}],"frecuent_topics":{"transformer":13,"pytorch":9,"nlp":4,"deep-learning":4,"machine-learning":4}}