{"category_type":"topic","category_name":"jax","repos_data":[{"full_name":"fchollet/keras","description":"Deep Learning for humans","topics":["deep-learning","tensorflow","neural-networks","machine-learning","data-science","python","jax","pytorch"],"created_at":"2015-03-28T00:35:42Z","pushed_at":"2025-12-19T02:08:41Z","stargazers_count":63648,"language":"Python"},{"full_name":"keras-team/keras","description":"Deep Learning for humans","topics":["deep-learning","tensorflow","neural-networks","machine-learning","data-science","python","jax","pytorch"],"created_at":"2015-03-28T00:35:42Z","pushed_at":"2025-12-19T02:08:41Z","stargazers_count":63648,"language":"Python"},{"full_name":"google/jax","description":"Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more","topics":["jax"],"created_at":"2018-10-25T21:25:02Z","pushed_at":"2025-12-19T15:52:51Z","stargazers_count":34369,"language":"Python"},{"full_name":"arogozhnikov/einops","description":"Flexible and powerful tensor operations for readable and reliable code (for pytorch, jax, TF and others)","topics":["deep-learning","pytorch","tensorflow","numpy","cupy","tensor","jax","einops","mlx"],"created_at":"2018-09-22T00:45:08Z","pushed_at":"2025-11-24T08:47:28Z","stargazers_count":9323,"language":"Python"},{"full_name":"google/trax","description":"Trax â€” Deep Learning with Clear Code and Speed","topics":["jax","numpy","deep-learning","deep-reinforcement-learning","machine-learning","transformer","reinforcement-learning"],"created_at":"2019-10-05T15:09:14Z","pushed_at":"2025-09-26T14:37:32Z","stargazers_count":8294,"language":"Python"},{"full_name":"google/flax","description":"Flax is a neural network library for JAX that is designed for flexibility.","topics":["jax"],"created_at":"2020-01-10T09:48:37Z","pushed_at":"2025-12-18T22:35:39Z","stargazers_count":6983,"language":"Python"},{"full_name":"NVIDIA/TransformerEngine","description":"A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit and 4-bit floating point (FP8 and FP4) precision on Hopper, Ada and Blackwell GPUs, to provide better performance with lower memory utilization in both training and inference.","topics":["cuda","deep-learning","gpu","machine-learning","python","pytorch","fp8","jax","fp4"],"created_at":"2022-09-20T15:20:26Z","pushed_at":"2025-12-19T02:08:03Z","stargazers_count":3011,"language":"Shell"},{"full_name":"AI-Hypercomputer/maxtext","description":"A simple, performant and scalable Jax LLM!","topics":["large-language-models","llm","gpt","deepseek","fine-tuning","gemma2","gemma3","jax","llama2","llama3"],"created_at":"2023-02-28T19:47:29Z","pushed_at":"2025-12-19T03:46:46Z","stargazers_count":2048,"language":"Python"},{"full_name":"n2cholas/awesome-jax","description":"JAX - A curated list of resources https://github.com/google/jax","topics":["awesome","awesome-list","jax","machine-learning","neural-network","deep-learning","xla","autograd","numpy"],"created_at":"2020-12-20T20:14:52Z","pushed_at":"2025-09-02T02:27:09Z","stargazers_count":1991,"language":"unknown"},{"full_name":"synsense/rockpool","description":"A machine learning library for spiking neural networks. Supports training with both torch and jax pipelines, and deployment to neuromorphic hardware.","topics":["deployment","jax","machine-learning","neuromorphic","pytorch","snn"],"created_at":"2021-09-09T15:44:24Z","pushed_at":"2025-12-18T02:25:47Z","stargazers_count":75,"language":"Shell"}],"frecuent_topics":{"jax":10,"deep-learning":6,"machine-learning":6,"pytorch":5,"tensorflow":3}}