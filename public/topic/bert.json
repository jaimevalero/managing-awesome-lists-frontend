{"category_type":"topic","category_name":"bert","repos_data":[{"full_name":"huggingface/transformers","description":"ðŸ¤— Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.","topics":["nlp","natural-language-processing","pytorch","language-model","tensorflow","bert","language-models","pytorch-transformers","nlp-library","transformer"],"created_at":"2018-10-29T13:56:00Z","pushed_at":"2024-09-13T12:59:35Z","stargazers_count":131963,"language":"Python"},{"full_name":"deepset-ai/haystack","description":":mag: AI orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.","topics":["nlp","question-answering","bert","language-model","pytorch","semantic-search","squad","information-retrieval","summarization","transformers"],"created_at":"2019-11-14T09:05:28Z","pushed_at":"2024-09-13T08:03:30Z","stargazers_count":16679,"language":"Python"},{"full_name":"huggingface/tokenizers","description":"ðŸ’¥ Fast State-of-the-Art Tokenizers optimized for Research and Production","topics":["nlp","natural-language-processing","natural-language-understanding","language-model","transformers","bert","gpt"],"created_at":"2019-11-01T17:52:20Z","pushed_at":"2024-09-03T22:28:12Z","stargazers_count":8882,"language":"Rust"},{"full_name":"codertimo/BERT-pytorch","description":"Google AI 2018 BERT pytorch implementation","topics":["bert","transformer","pytorch","nlp","language-model"],"created_at":"2018-10-15T12:58:15Z","pushed_at":"2023-09-15T12:57:08Z","stargazers_count":6159,"language":"Python"},{"full_name":"NVIDIA/FasterTransformer","description":"Transformer related optimization, including BERT, GPT","topics":["pytorch","transformer","gpt","bert"],"created_at":"2021-04-02T21:36:33Z","pushed_at":"2024-03-27T11:25:30Z","stargazers_count":5770,"language":"CMake"},{"full_name":"guillaume-be/rust-bert","description":"Rust native ready-to-use NLP pipelines and transformer-based models (BERT, DistilBERT, GPT2,...)","topics":["deep-learning","nlp","transformer","bert","rust-lang","rust","machine-learning","ner","sentiment-analysis","question-answering"],"created_at":"2020-01-25T09:40:07Z","pushed_at":"2024-08-19T20:27:20Z","stargazers_count":2576,"language":"Rust"},{"full_name":"jsalt18-sentence-repl/jiant","description":"jiant is an nlp toolkit","topics":["nlp","sentence-representation","bert","multitask-learning","transformers","transfer-learning"],"created_at":"2018-06-18T18:12:47Z","pushed_at":"2023-07-06T22:00:38Z","stargazers_count":1637,"language":"Python"},{"full_name":"kamalkraj/BERT-NER","description":"Pytorch-Named-Entity-Recognition-with-BERT","topics":["bert","named-entity-recognition","pytorch","conll-2003","cpp11","bert-ner","inference","curl","postman","pretrained-models"],"created_at":"2019-02-24T10:40:46Z","pushed_at":"2021-05-06T19:38:36Z","stargazers_count":1198,"language":"Python"},{"full_name":"seriousran/awesome-qa","description":"ðŸ˜Ž A curated list of the Question Answering (QA)","topics":["question-answering","nlp","machine-comprehension","awesome","awesome-list","squad","bert","watson","deepqa"],"created_at":"2018-07-20T02:31:32Z","pushed_at":"2022-01-13T02:23:01Z","stargazers_count":750,"language":"unknown"},{"full_name":"asyml/texar-pytorch","description":"Integrating the Best of TF into PyTorch, for Machine Learning, Natural Language Processing, and Text Generation.  This is part of the CASL project: http://casl-project.ai/","topics":["machine-learning","natural-language-processing","pytorch","deep-learning","text-generation","python","machine-translation","dialog-systems","texar","bert"],"created_at":"2019-03-08T01:04:09Z","pushed_at":"2022-04-14T01:26:58Z","stargazers_count":745,"language":"Python"},{"full_name":"barissayil/SentimentAnalysis","description":"Sentiment analysis neural network trained by fine-tuning BERT, ALBERT, or DistilBERT on the Stanford Sentiment Treebank.","topics":["bert","nlp","machine-learning","pytorch","pytorch-implementation","vuejs","flask","transformer","huggingface","huggingface-transformer"],"created_at":"2019-12-27T11:54:06Z","pushed_at":"2023-06-12T21:34:19Z","stargazers_count":363,"language":"Python"},{"full_name":"nlpodyssey/cybertron","description":"Cybertron: the home planet of the Transformers in Go","topics":["bart","bert","machine-translation","question-answering","zero-shot-classification","bert-as-service","transformers","huggingface","text-classification","named-entity-recognition"],"created_at":"2022-06-21T13:45:54Z","pushed_at":"2024-06-08T19:22:45Z","stargazers_count":279,"language":"Go"},{"full_name":"backprop-ai/backprop","description":"Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.","topics":["natural-language-processing","nlp","question-answering","bert","language-model","text-classification","multilingual-models","image-classification","fine-tuning","transfer-learning"],"created_at":"2020-10-30T15:25:14Z","pushed_at":"2021-05-03T09:15:25Z","stargazers_count":242,"language":"Python"}],"frecuent_topics":{"bert":13,"nlp":9,"pytorch":7,"language-model":5,"transformer":5}}