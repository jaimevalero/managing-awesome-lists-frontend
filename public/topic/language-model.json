{"category_type":"topic","category_name":"language-model","repos_data":[{"full_name":"huggingface/transformers","description":"ü§ó Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.","topics":["nlp","natural-language-processing","pytorch","language-model","tensorflow","bert","language-models","pytorch-transformers","nlp-library","transformer"],"created_at":"2018-10-29T13:56:00Z","pushed_at":"2024-11-29T23:13:12Z","stargazers_count":135733,"language":"Python"},{"full_name":"deepset-ai/haystack","description":"AI orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.","topics":["nlp","question-answering","bert","language-model","pytorch","semantic-search","squad","information-retrieval","summarization","transformers"],"created_at":"2019-11-14T09:05:28Z","pushed_at":"2024-12-01T01:47:18Z","stargazers_count":17941,"language":"Python"},{"full_name":"huggingface/tokenizers","description":"üí• Fast State-of-the-Art Tokenizers optimized for Research and Production","topics":["nlp","natural-language-processing","natural-language-understanding","language-model","transformers","bert","gpt"],"created_at":"2019-11-01T17:52:20Z","pushed_at":"2024-11-27T15:01:51Z","stargazers_count":9092,"language":"Rust"},{"full_name":"EleutherAI/lm-evaluation-harness","description":"A framework for few-shot evaluation of language models.","topics":["evaluation-framework","language-model","transformer"],"created_at":"2020-08-28T00:09:15Z","pushed_at":"2024-12-01T12:31:51Z","stargazers_count":7099,"language":"Python"},{"full_name":"EleutherAI/gpt-neox","description":"An implementation of model parallel autoregressive transformers on GPUs, based on the Megatron and DeepSpeed libraries","topics":["deepspeed-library","gpt-3","transformers","language-model"],"created_at":"2020-12-22T14:37:54Z","pushed_at":"2024-11-29T20:49:32Z","stargazers_count":6967,"language":"Python"},{"full_name":"OpenNMT/OpenNMT-py","description":"Open Source Neural Machine Translation and (Large) Language Models in PyTorch","topics":["deep-learning","pytorch","machine-translation","neural-machine-translation","language-model","llms"],"created_at":"2017-02-22T19:01:50Z","pushed_at":"2024-06-27T17:56:28Z","stargazers_count":6783,"language":"Python"},{"full_name":"codertimo/BERT-pytorch","description":"Google AI 2018 BERT pytorch implementation","topics":["bert","transformer","pytorch","nlp","language-model"],"created_at":"2018-10-15T12:58:15Z","pushed_at":"2023-09-15T12:57:08Z","stargazers_count":6233,"language":"Python"},{"full_name":"langroid/langroid","description":"Harness LLMs with Multi-Agent Programming","topics":["agents","chatgpt","gpt","gpt-4","gpt4","language-model","llm","llm-agent","multi-agent-systems","openai-api"],"created_at":"2023-04-16T20:47:28Z","pushed_at":"2024-11-27T00:12:52Z","stargazers_count":2728,"language":"Python"},{"full_name":"huggingface/pytorch-openai-transformer-lm","description":"üê•A PyTorch implementation of OpenAI's finetuned transformer language model with a script to import the weights pre-trained by OpenAI","topics":["neural-networks","pytorch","openai","language-model","transformer"],"created_at":"2018-06-13T14:02:41Z","pushed_at":"2021-08-09T16:17:12Z","stargazers_count":1512,"language":"Python"},{"full_name":"llm-jp/awesome-japanese-llm","description":"Êó•Êú¨Ë™ûLLM„Åæ„Å®„ÇÅ - Overview of Japanese LLMs","topics":["language-model","language-models","large-language-model","large-language-models","llm","llms","japanese","japanese-language","vision-and-language","foundation-models"],"created_at":"2023-07-09T04:36:38Z","pushed_at":"2024-12-01T13:05:28Z","stargazers_count":1032,"language":"TypeScript"},{"full_name":"LiyuanLucasLiu/LM-LSTM-CRF","description":"Empower Sequence Labeling with Task-Aware Language Model","topics":["ner","language-model","crf","sequence-labeling","pytorch"],"created_at":"2017-09-12T19:24:49Z","pushed_at":"2022-06-22T20:29:39Z","stargazers_count":846,"language":"Python"},{"full_name":"Stonesjtu/Pytorch-NCE","description":"The Noise Contrastive Estimation for softmax output written in Pytorch","topics":["pytorch","nce","language-model","nce-criterion","importance-sampling","speedup","softmax"],"created_at":"2017-05-19T01:18:57Z","pushed_at":"2019-11-06T14:40:59Z","stargazers_count":317,"language":"Python"},{"full_name":"feedly/transfer-nlp","description":"NLP library designed for reproducible experimentation management","topics":["nlp","transfer-learning","framework","playground","natural-language-understanding","language-model","pytorch"],"created_at":"2019-03-12T20:00:31Z","pushed_at":"2024-07-25T10:16:22Z","stargazers_count":292,"language":"Python"},{"full_name":"L0SG/relational-rnn-pytorch","description":"An implementation of DeepMind's Relational Recurrent Neural Networks (NeurIPS 2018) in PyTorch.","topics":["pytorch","language-model","word-language-model","language-modeling","deep-learning","recurrent-neural-networks","deepmind","transformer","self-attention"],"created_at":"2018-08-21T07:57:41Z","pushed_at":"2018-12-27T05:38:23Z","stargazers_count":245,"language":"Python"},{"full_name":"backprop-ai/backprop","description":"Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.","topics":["natural-language-processing","nlp","question-answering","bert","language-model","text-classification","multilingual-models","image-classification","fine-tuning","transfer-learning"],"created_at":"2020-10-30T15:25:14Z","pushed_at":"2021-05-03T09:15:25Z","stargazers_count":243,"language":"Python"},{"full_name":"rylans/getlang","description":"Natural language detection package in pure Go","topics":["nlp","natural-language","language-model"],"created_at":"2018-03-01T21:27:30Z","pushed_at":"2020-12-27T07:47:21Z","stargazers_count":171,"language":"Go"},{"full_name":"rdspring1/PyTorch_GBW_LM","description":"PyTorch Language Model for 1-Billion Word (LM1B / GBW) Dataset","topics":["pytorch","language-model","lstm","deep-learning","gpu","machine-learning","nlp","torch","torch-gbw"],"created_at":"2017-11-15T14:54:57Z","pushed_at":"2019-08-22T22:08:16Z","stargazers_count":123,"language":"Python"},{"full_name":"batzner/tensorlm","description":"Wrapper library for text generation / language models at character and word level with RNNs in TensorFlow","topics":["language-model","tensorflow","tensorflow-library","char-rnn","char-lm","nlp"],"created_at":"2017-08-20T12:54:14Z","pushed_at":"2022-06-21T21:07:11Z","stargazers_count":61,"language":"Python"}],"frecuent_topics":{"language-model":18,"pytorch":10,"nlp":9,"bert":5,"transformer":5}}