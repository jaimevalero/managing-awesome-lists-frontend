{"category_type":"topic","category_name":"language-model","repos_data":[{"full_name":"arc53/DocsGPT","description":"Private AI platform for agents, assistants and enterprise search. Built-in Agent Builder, Deep research, Document analysis, Multi-model support, and API connectivity for agents.","topics":["ai","python","natural-language-processing","react","chatgpt","docsgpt","information-retrieval","language-model","llm","machine-learning"],"created_at":"2023-02-02T11:03:23Z","pushed_at":"2025-12-19T16:13:33Z","stargazers_count":17530,"language":"Dockerfile"},{"full_name":"neuml/txtai","description":"üí° All-in-one AI framework for semantic search, LLM orchestration and language model workflows","topics":["python","search","nlp","semantic-search","vector-search","txtai","llm","vector-database","language-model","transformers"],"created_at":"2020-08-09T19:14:59Z","pushed_at":"2025-12-15T18:35:56Z","stargazers_count":11946,"language":"Python"},{"full_name":"EleutherAI/lm-evaluation-harness","description":"A framework for few-shot evaluation of language models.","topics":["evaluation-framework","language-model","transformer"],"created_at":"2020-08-28T00:09:15Z","pushed_at":"2025-12-18T21:45:37Z","stargazers_count":10976,"language":"Python"},{"full_name":"huggingface/tokenizers","description":"üí• Fast State-of-the-Art Tokenizers optimized for Research and Production","topics":["nlp","natural-language-processing","natural-language-understanding","language-model","transformers","bert","gpt"],"created_at":"2019-11-01T17:52:20Z","pushed_at":"2025-12-16T10:52:44Z","stargazers_count":10321,"language":"Rust"},{"full_name":"EleutherAI/gpt-neox","description":"An implementation of model parallel autoregressive transformers on GPUs, based on the Megatron and DeepSpeed libraries","topics":["deepspeed-library","gpt-3","transformers","language-model"],"created_at":"2020-12-22T14:37:54Z","pushed_at":"2025-12-10T21:14:29Z","stargazers_count":7353,"language":"Python"},{"full_name":"OpenNMT/OpenNMT-py","description":"Open Source Neural Machine Translation and (Large) Language Models in PyTorch","topics":["deep-learning","pytorch","machine-translation","neural-machine-translation","language-model","llms"],"created_at":"2017-02-22T19:01:50Z","pushed_at":"2025-10-14T17:32:07Z","stargazers_count":6979,"language":"Python"},{"full_name":"codertimo/BERT-pytorch","description":"Google AI 2018 BERT pytorch implementation","topics":["bert","transformer","pytorch","nlp","language-model"],"created_at":"2018-10-15T12:58:15Z","pushed_at":"2023-09-15T12:57:08Z","stargazers_count":6508,"language":"Python"},{"full_name":"langroid/langroid","description":"Harness LLMs with Multi-Agent Programming","topics":["agents","chatgpt","gpt","gpt-4","gpt4","language-model","llm","llm-agent","multi-agent-systems","openai-api"],"created_at":"2023-04-16T20:47:28Z","pushed_at":"2025-11-24T21:02:31Z","stargazers_count":3805,"language":"Python"},{"full_name":"huggingface/pytorch-openai-transformer-lm","description":"üê•A PyTorch implementation of OpenAI's finetuned transformer language model with a script to import the weights pre-trained by OpenAI","topics":["neural-networks","pytorch","openai","language-model","transformer"],"created_at":"2018-06-13T14:02:41Z","pushed_at":"2021-08-09T16:17:12Z","stargazers_count":1521,"language":"Python"},{"full_name":"llm-jp/awesome-japanese-llm","description":"Êó•Êú¨Ë™ûLLM„Åæ„Å®„ÇÅ - Overview of Japanese LLMs","topics":["language-model","language-models","large-language-model","large-language-models","llm","llms","japanese","japanese-language","vision-and-language","foundation-models"],"created_at":"2023-07-09T04:36:38Z","pushed_at":"2025-12-14T04:46:40Z","stargazers_count":1324,"language":"TypeScript"},{"full_name":"LiyuanLucasLiu/LM-LSTM-CRF","description":"Empower Sequence Labeling with Task-Aware Language Model","topics":["ner","language-model","crf","sequence-labeling","pytorch"],"created_at":"2017-09-12T19:24:49Z","pushed_at":"2022-06-22T20:29:39Z","stargazers_count":849,"language":"Python"},{"full_name":"Stonesjtu/Pytorch-NCE","description":"The Noise Contrastive Estimation for softmax output written in Pytorch","topics":["pytorch","nce","language-model","nce-criterion","importance-sampling","speedup","softmax"],"created_at":"2017-05-19T01:18:57Z","pushed_at":"2019-11-06T14:40:59Z","stargazers_count":319,"language":"Python"},{"full_name":"feedly/transfer-nlp","description":"NLP library designed for reproducible experimentation management","topics":["nlp","transfer-learning","framework","playground","natural-language-understanding","language-model","pytorch"],"created_at":"2019-03-12T20:00:31Z","pushed_at":"2024-07-25T10:16:22Z","stargazers_count":294,"language":"Python"},{"full_name":"L0SG/relational-rnn-pytorch","description":"An implementation of DeepMind's Relational Recurrent Neural Networks (NeurIPS 2018) in PyTorch.","topics":["pytorch","language-model","word-language-model","language-modeling","deep-learning","recurrent-neural-networks","deepmind","transformer","self-attention"],"created_at":"2018-08-21T07:57:41Z","pushed_at":"2018-12-27T05:38:23Z","stargazers_count":248,"language":"Python"},{"full_name":"backprop-ai/backprop","description":"Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.","topics":["natural-language-processing","nlp","question-answering","bert","language-model","text-classification","multilingual-models","image-classification","fine-tuning","transfer-learning"],"created_at":"2020-10-30T15:25:14Z","pushed_at":"2021-05-03T09:15:25Z","stargazers_count":242,"language":"Python"},{"full_name":"rylans/getlang","description":"Natural language detection package in pure Go","topics":["nlp","natural-language","language-model"],"created_at":"2018-03-01T21:27:30Z","pushed_at":"2020-12-27T07:47:21Z","stargazers_count":175,"language":"Go"},{"full_name":"rdspring1/PyTorch_GBW_LM","description":"PyTorch Language Model for 1-Billion Word (LM1B / GBW) Dataset","topics":["pytorch","language-model","lstm","deep-learning","gpu","machine-learning","nlp","torch","torch-gbw"],"created_at":"2017-11-15T14:54:57Z","pushed_at":"2019-08-22T22:08:16Z","stargazers_count":123,"language":"Python"},{"full_name":"batzner/tensorlm","description":"Wrapper library for text generation / language models at character and word level with RNNs in TensorFlow","topics":["language-model","tensorflow","tensorflow-library","char-rnn","char-lm","nlp"],"created_at":"2017-08-20T12:54:14Z","pushed_at":"2022-06-21T21:07:11Z","stargazers_count":60,"language":"Python"}],"frecuent_topics":{"language-model":18,"nlp":8,"pytorch":8,"llm":4,"transformer":4}}