{"category_type":"topic","category_name":"llm-serving","repos_data":[{"full_name":"vllm-project/vllm","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","topics":["gpt","llm","pytorch","llmops","mlops","model-serving","transformer","llm-serving","inference","llama"],"created_at":"2023-02-09T11:23:20Z","pushed_at":"2024-05-02T04:29:55Z","stargazers_count":18791,"language":"Python"},{"full_name":"bentoml/OpenLLM","description":"Run any open-source LLMs, such as Llama 2, Mistral, as OpenAI compatible API endpoint in the cloud.","topics":["llm","llmops","model-inference","falcon","fine-tuning","stablelm","llm-serving","llama","mpt","vicuna"],"created_at":"2023-04-19T00:27:52Z","pushed_at":"2024-04-30T19:06:47Z","stargazers_count":8822,"language":"Python"}],"frecuent_topics":{"llm":2,"llmops":2,"llm-serving":2,"llama":2,"gpt":1}}