{"category_type":"topic","category_name":"llm-serving","repos_data":[{"full_name":"vllm-project/vllm","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","topics":["gpt","llm","pytorch","llmops","mlops","model-serving","transformer","llm-serving","inference","llama"],"created_at":"2023-02-09T11:23:20Z","pushed_at":"2024-08-16T04:47:13Z","stargazers_count":24832,"language":"Python"},{"full_name":"bentoml/OpenLLM","description":"Run any open-source LLMs, such as Llama 3.1, Gemma, as OpenAI compatible API endpoint in the cloud.","topics":["llm","llmops","model-inference","falcon","fine-tuning","stablelm","llm-serving","llama","mpt","vicuna"],"created_at":"2023-04-19T00:27:52Z","pushed_at":"2024-08-12T16:56:29Z","stargazers_count":9576,"language":"Python"},{"full_name":"bentoml/BentoML","description":"The easiest way to serve AI apps and models - Build reliable Inference APIs, LLM apps, Multi-model chains, RAG service, and much more!","topics":["model-serving","mlops","llmops","generative-ai","llm-inference","model-inference-service","inference-platform","deep-learning","llm-serving","machine-learning"],"created_at":"2019-04-02T01:39:27Z","pushed_at":"2024-08-16T07:30:27Z","stargazers_count":6894,"language":"Python"},{"full_name":"mani-kantap/llm-inference-solutions","description":"A collection of all available inference solutions for the LLMs","topics":["llm-inference","llm-serving","llmops"],"created_at":"2023-07-23T20:39:23Z","pushed_at":"2024-04-25T21:01:31Z","stargazers_count":56,"language":"unknown"}],"frecuent_topics":{"llmops":4,"llm-serving":4,"llm":2,"mlops":2,"model-serving":2}}