{"category_type":"topic","category_name":"llm-serving","repos_data":[{"full_name":"vllm-project/vllm","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","topics":["gpt","llm","pytorch","model-serving","transformer","llm-serving","inference","llama","amd","cuda"],"created_at":"2023-02-09T11:23:20Z","pushed_at":"2025-12-19T16:08:53Z","stargazers_count":65779,"language":"Python"},{"full_name":"sgl-project/sglang","description":"SGLang is a fast serving framework for large language models and vision language models.","topics":["cuda","inference","llama","llava","llm","llm-serving","moe","pytorch","transformer","vlm"],"created_at":"2024-01-08T04:15:52Z","pushed_at":"2025-12-19T16:12:00Z","stargazers_count":21743,"language":"Python"},{"full_name":"NVIDIA/TensorRT-LLM","description":"TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and supports state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in a performant way.","topics":["blackwell","cuda","moe","pytorch","llm-serving"],"created_at":"2023-08-16T17:14:27Z","pushed_at":"2025-12-19T14:14:27Z","stargazers_count":12425,"language":"Python"},{"full_name":"bentoml/OpenLLM","description":"Run any open-source LLMs, such as DeepSeek and Llama, as OpenAI compatible API endpoint in the cloud.","topics":["llm","llmops","model-inference","fine-tuning","llm-serving","llama","vicuna","bentoml","llama2","llm-inference"],"created_at":"2023-04-19T00:27:52Z","pushed_at":"2025-12-15T16:55:53Z","stargazers_count":12006,"language":"Python"},{"full_name":"bentoml/BentoML","description":"The easiest way to serve AI apps and models - Build Model Inference APIs, Job queues, LLM apps, Multi-model pipelines, and more!","topics":["model-serving","mlops","llmops","generative-ai","llm-inference","model-inference-service","inference-platform","deep-learning","llm-serving","machine-learning"],"created_at":"2019-04-02T01:39:27Z","pushed_at":"2026-01-12T07:22:02Z","stargazers_count":8357,"language":"Python"},{"full_name":"mani-kantap/llm-inference-solutions","description":"A collection of all available inference solutions for the LLMs","topics":["llm-inference","llm-serving","llmops"],"created_at":"2023-07-23T20:39:23Z","pushed_at":"2025-03-01T13:49:13Z","stargazers_count":93,"language":"unknown"}],"frecuent_topics":{"llm-serving":6,"llm":3,"pytorch":3,"llama":3,"cuda":3}}