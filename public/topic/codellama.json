{"category_type":"topic","category_name":"codellama","repos_data":[{"full_name":"InternLM/lmdeploy","description":"LMDeploy is a toolkit for compressing, deploying, and serving LLMs.","topics":["cuda-kernels","deepspeed","fastertransformer","llm-inference","turbomind","internlm","llama","llm","codellama","llama2"],"created_at":"2023-06-15T12:38:06Z","pushed_at":"2024-08-16T04:50:37Z","stargazers_count":3882,"language":"Python"},{"full_name":"yusufcanb/tlm","description":"Local CLI Copilot, powered by CodeLLaMa. ðŸ’»ðŸ¦™","topics":["llm","codellama","bash","powershell","llama3","zsh"],"created_at":"2024-02-14T12:00:13Z","pushed_at":"2024-07-13T22:36:01Z","stargazers_count":1166,"language":"Dockerfile"}],"frecuent_topics":{"llm":2,"codellama":2,"cuda-kernels":1,"deepspeed":1,"fastertransformer":1}}