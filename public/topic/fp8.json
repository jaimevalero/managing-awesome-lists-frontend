{"category_type":"topic","category_name":"fp8","repos_data":[{"full_name":"NVIDIA/TransformerEngine","description":"A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit and 4-bit floating point (FP8 and FP4) precision on Hopper, Ada and Blackwell GPUs, to provide better performance with lower memory utilization in both training and inference.","topics":["cuda","deep-learning","gpu","machine-learning","python","pytorch","fp8","jax","fp4"],"created_at":"2022-09-20T15:20:26Z","pushed_at":"2025-12-19T02:08:03Z","stargazers_count":3011,"language":"Shell"}],"frecuent_topics":{"cuda":1,"deep-learning":1,"gpu":1,"machine-learning":1,"python":1}}