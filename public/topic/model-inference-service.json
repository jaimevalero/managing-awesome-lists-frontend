{"category_type":"topic","category_name":"model-inference-service","repos_data":[{"full_name":"bentoml/BentoML","description":"The easiest way to serve AI apps and models - Build Model Inference APIs, Job queues, LLM apps, Multi-model pipelines, and more!","topics":["model-serving","mlops","llmops","generative-ai","llm-inference","model-inference-service","inference-platform","deep-learning","llm-serving","machine-learning"],"created_at":"2019-04-02T01:39:27Z","pushed_at":"2025-04-10T04:50:29Z","stargazers_count":7584,"language":"Python"}],"frecuent_topics":{"model-serving":1,"mlops":1,"llmops":1,"generative-ai":1,"llm-inference":1}}