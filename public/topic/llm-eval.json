{"category_type":"topic","category_name":"llm-eval","repos_data":[{"full_name":"typpo/promptfoo","description":"Test your prompts, agents, and RAGs. AI Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.","topics":["llm","prompt-engineering","prompts","llmops","prompt-testing","testing","rag","evaluation","evaluation-framework","llm-eval"],"created_at":"2023-04-28T15:48:49Z","pushed_at":"2025-12-19T16:01:54Z","stargazers_count":9484,"language":"TypeScript"},{"full_name":"Arize-ai/phoenix","description":"AI Observability & Evaluation","topics":["llmops","ai-monitoring","ai-observability","llm-eval","aiengineering","datasets","agents","llms","prompt-engineering","anthropic"],"created_at":"2022-11-09T23:44:35Z","pushed_at":"2025-12-19T15:22:39Z","stargazers_count":7977,"language":"Python"},{"full_name":"Giskard-AI/giskard-oss","description":"üê¢ Open-Source Evaluation & Testing library for LLM Agents","topics":["mlops","ml-validation","ml-testing","ai-testing","llmops","responsible-ai","fairness-ai","trustworthy-ai","llm-eval","llm-evaluation"],"created_at":"2022-03-06T21:45:37Z","pushed_at":"2025-11-18T11:06:33Z","stargazers_count":5064,"language":"Shell"}],"frecuent_topics":{"llmops":3,"llm-eval":3,"prompt-engineering":2,"llm":1,"prompts":1}}