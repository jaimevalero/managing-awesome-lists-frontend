{"category_type":"topic","category_name":"moe","repos_data":[{"full_name":"sgl-project/sglang","description":"SGLang is a fast serving framework for large language models and vision language models.","topics":["cuda","inference","llama","llava","llm","llm-serving","moe","pytorch","transformer","vlm"],"created_at":"2024-01-08T04:15:52Z","pushed_at":"2025-12-19T16:12:00Z","stargazers_count":21743,"language":"Python"},{"full_name":"NVIDIA/TensorRT-LLM","description":"TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and supports state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in a performant way.","topics":["blackwell","cuda","moe","pytorch","llm-serving"],"created_at":"2023-08-16T17:14:27Z","pushed_at":"2025-12-19T14:14:27Z","stargazers_count":12425,"language":"Python"}],"frecuent_topics":{"cuda":2,"llm-serving":2,"moe":2,"pytorch":2,"inference":1}}