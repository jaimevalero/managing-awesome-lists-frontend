{"category_type":"topic","category_name":"evaluation-framework","repos_data":[{"full_name":"EleutherAI/lm-evaluation-harness","description":"A framework for few-shot evaluation of language models.","topics":["evaluation-framework","language-model","transformer"],"created_at":"2020-08-28T00:09:15Z","pushed_at":"2024-05-01T21:43:01Z","stargazers_count":5098,"language":"Python"},{"full_name":"typpo/promptfoo","description":"Test your prompts, models, and RAGs. Catch regressions and improve prompt quality. LLM evals for OpenAI, Azure, Anthropic, Gemini, Mistral, Llama, Bedrock, Ollama, and other local & private models with CI/CD integration.","topics":["llm","prompt-engineering","prompts","llmops","prompt-testing","testing","rag","evaluation","evaluation-framework","llm-eval"],"created_at":"2023-04-28T15:48:49Z","pushed_at":"2024-05-02T05:42:35Z","stargazers_count":2757,"language":"TypeScript"},{"full_name":"huggingface/lighteval","description":"LightEval is a lightweight LLM evaluation suite that Hugging Face has been using internally with the recently released LLM data processing library datatrove and LLM training library nanotron.","topics":["evaluation","evaluation-framework","evaluation-metrics","huggingface"],"created_at":"2024-01-26T13:15:39Z","pushed_at":"2024-05-01T05:56:26Z","stargazers_count":346,"language":"Python"}],"frecuent_topics":{"evaluation-framework":3,"evaluation":2,"language-model":1,"transformer":1,"llm":1}}