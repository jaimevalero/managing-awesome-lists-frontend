{"category_type":"topic","category_name":"evaluation-framework","repos_data":[{"full_name":"EleutherAI/lm-evaluation-harness","description":"A framework for few-shot evaluation of language models.","topics":["evaluation-framework","language-model","transformer"],"created_at":"2020-08-28T00:09:15Z","pushed_at":"2024-12-01T12:31:51Z","stargazers_count":7099,"language":"Python"},{"full_name":"typpo/promptfoo","description":"Test your prompts, agents, and RAGs. Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.","topics":["llm","prompt-engineering","prompts","llmops","prompt-testing","testing","rag","evaluation","evaluation-framework","llm-eval"],"created_at":"2023-04-28T15:48:49Z","pushed_at":"2024-12-02T04:03:49Z","stargazers_count":4888,"language":"TypeScript"},{"full_name":"huggingface/lighteval","description":"Lighteval is your all-in-one toolkit for evaluating LLMs across multiple backends","topics":["evaluation","evaluation-framework","evaluation-metrics","huggingface"],"created_at":"2024-01-26T13:15:39Z","pushed_at":"2024-11-29T18:36:37Z","stargazers_count":852,"language":"Python"},{"full_name":"Psycoy/MixEval","description":"The official evaluation suite and dynamic data release for MixEval.","topics":["benchmark","benchmark-mixture","benchmarking-framework","benchmarking-suite","evaluation","evaluation-framework","foundation-models","large-language-model","large-language-models","large-multimodal-models"],"created_at":"2024-06-01T10:50:50Z","pushed_at":"2024-11-10T02:23:50Z","stargazers_count":227,"language":"Python"}],"frecuent_topics":{"evaluation-framework":4,"evaluation":3,"language-model":1,"transformer":1,"llm":1}}