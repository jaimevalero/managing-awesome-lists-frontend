{"category_type":"topic","category_name":"evaluation-framework","repos_data":[{"full_name":"EleutherAI/lm-evaluation-harness","description":"A framework for few-shot evaluation of language models.","topics":["evaluation-framework","language-model","transformer"],"created_at":"2020-08-28T00:09:15Z","pushed_at":"2024-08-16T03:11:10Z","stargazers_count":6144,"language":"Python"},{"full_name":"typpo/promptfoo","description":"Test your prompts, agents, and RAGs. Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.","topics":["llm","prompt-engineering","prompts","llmops","prompt-testing","testing","rag","evaluation","evaluation-framework","llm-eval"],"created_at":"2023-04-28T15:48:49Z","pushed_at":"2024-08-15T23:59:18Z","stargazers_count":3897,"language":"TypeScript"},{"full_name":"huggingface/lighteval","description":"LightEval is a lightweight LLM evaluation suite that Hugging Face has been using internally with the recently released LLM data processing library datatrove and LLM training library nanotron.","topics":["evaluation","evaluation-framework","evaluation-metrics","huggingface"],"created_at":"2024-01-26T13:15:39Z","pushed_at":"2024-08-15T05:47:58Z","stargazers_count":519,"language":"Python"},{"full_name":"Psycoy/MixEval","description":"The official evaluation suite and dynamic data release for MixEval.","topics":["benchmark","benchmark-mixture","benchmarking-framework","benchmarking-suite","evaluation","evaluation-framework","foundation-models","large-language-model","large-language-models","large-multimodal-models"],"created_at":"2024-06-01T10:50:50Z","pushed_at":"2024-08-11T13:23:31Z","stargazers_count":192,"language":"Python"}],"frecuent_topics":{"evaluation-framework":4,"evaluation":3,"language-model":1,"transformer":1,"llm":1}}