{"category_type":"topic","category_name":"llm-inference","repos_data":[{"full_name":"nomic-ai/gpt4all","description":"gpt4all: run open-source LLMs anywhere","topics":["llm-inference"],"created_at":"2023-03-27T18:49:32Z","pushed_at":"2024-05-01T20:15:48Z","stargazers_count":64739,"language":"Python"},{"full_name":"InternLM/lmdeploy","description":"LMDeploy is a toolkit for compressing, deploying, and serving LLMs.","topics":["cuda-kernels","deepspeed","fastertransformer","llm-inference","turbomind","internlm","llama","llm","codellama","llama2"],"created_at":"2023-06-15T12:38:06Z","pushed_at":"2024-05-02T03:43:33Z","stargazers_count":2371,"language":"Python"}],"frecuent_topics":{"llm-inference":2,"cuda-kernels":1,"deepspeed":1,"fastertransformer":1,"turbomind":1}}