{"category_type":"topic","category_name":"llm-inference","repos_data":[{"full_name":"bentoml/BentoML","description":"The easiest way to serve AI apps and models - Build reliable Inference APIs, LLM apps, Multi-model chains, RAG service, and much more!","topics":["model-serving","mlops","llmops","generative-ai","llm-inference","model-inference-service","inference-platform","deep-learning","llm-serving","machine-learning"],"created_at":"2019-04-02T01:39:27Z","pushed_at":"2024-09-13T10:06:16Z","stargazers_count":6978,"language":"Python"},{"full_name":"InternLM/lmdeploy","description":"LMDeploy is a toolkit for compressing, deploying, and serving LLMs.","topics":["cuda-kernels","deepspeed","fastertransformer","llm-inference","turbomind","internlm","llama","llm","codellama","llama2"],"created_at":"2023-06-15T12:38:06Z","pushed_at":"2024-09-13T03:12:32Z","stargazers_count":4159,"language":"Python"},{"full_name":"DefTruth/Awesome-LLM-Inference","description":"ðŸ“–A curated list of Awesome LLM Inference Paper with codes, TensorRT-LLM, vLLM, streaming-llm, AWQ, SmoothQuant, WINT8/4, Continuous Batching, FlashAttention, PagedAttention etc.","topics":["flash-attention","flash-attention-2","paged-attention","tensorrt-llm","vllm","awesome-llm","sora","llm","llm-inference","llms"],"created_at":"2023-08-27T02:32:15Z","pushed_at":"2024-09-09T01:25:29Z","stargazers_count":2444,"language":"unknown"},{"full_name":"mani-kantap/llm-inference-solutions","description":"A collection of all available inference solutions for the LLMs","topics":["llm-inference","llm-serving","llmops"],"created_at":"2023-07-23T20:39:23Z","pushed_at":"2024-08-31T15:43:44Z","stargazers_count":65,"language":"unknown"}],"frecuent_topics":{"llm-inference":4,"llmops":2,"llm-serving":2,"llm":2,"model-serving":1}}