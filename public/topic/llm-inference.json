{"category_type":"topic","category_name":"llm-inference","repos_data":[{"full_name":"bentoml/BentoML","description":"The easiest way to serve AI apps and models - Build Model Inference APIs, Job queues, LLM apps, Multi-model pipelines, and more!","topics":["model-serving","mlops","llmops","generative-ai","llm-inference","model-inference-service","inference-platform","deep-learning","llm-serving","machine-learning"],"created_at":"2019-04-02T01:39:27Z","pushed_at":"2026-02-08T03:01:26Z","stargazers_count":8415,"language":"Python"},{"full_name":"Michael-A-Kuykendall/shimmy","description":"⚡ Python-free Rust inference server — OpenAI-API compatible. GGUF + SafeTensors, hot model swap, auto-discovery, single binary. FREE now, FREE forever.","topics":["llama","llamacpp","llm-inference","ollama-api","command-line-tool","gguf","inference-server","local-ai","lora","machine-learning"],"created_at":"2025-08-28T22:55:46Z","pushed_at":"2026-01-16T23:01:22Z","stargazers_count":3538,"language":"Rust"}],"frecuent_topics":{"llm-inference":2,"machine-learning":2,"model-serving":1,"mlops":1,"llmops":1}}