{"category_type":"topic","category_name":"mistral","repos_data":[{"full_name":"jmorganca/ollama","description":"Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models.","topics":["llama","llm","llama2","llms","go","golang","ollama","mistral","gemma","llama3"],"created_at":"2023-06-26T19:39:32Z","pushed_at":"2024-12-01T05:22:08Z","stargazers_count":100365,"language":"TypeScript"},{"full_name":"ollama/ollama","description":"Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models.","topics":["llama","llm","llama2","llms","go","golang","ollama","mistral","gemma","llama3"],"created_at":"2023-06-26T19:39:32Z","pushed_at":"2024-12-01T05:22:08Z","stargazers_count":100364,"language":"TypeScript"},{"full_name":"unslothai/unsloth","description":"Finetune Llama 3.2, Mistral, Phi, Qwen 2.5 & Gemma LLMs 2-5x faster with 80% less memory","topics":["ai","finetuning","fine-tuning","llama","llms","lora","mistral","qlora","gemma","llama3"],"created_at":"2023-11-29T16:50:09Z","pushed_at":"2024-12-01T10:36:19Z","stargazers_count":18753,"language":"Python"},{"full_name":"linkedin/Liger-Kernel","description":"Efficient Triton Kernels for LLM Training","topics":["llm-training","triton","finetuning","gemma2","llama","llama3","llms","mistral","phi3","triton-kernels"],"created_at":"2024-08-06T17:47:52Z","pushed_at":"2025-02-09T14:52:05Z","stargazers_count":4378,"language":"Makefile"}],"frecuent_topics":{"llama":4,"llms":4,"mistral":4,"llama3":4,"gemma":3}}