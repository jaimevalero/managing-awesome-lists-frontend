{"category_type":"topic","category_name":"model-deployment","repos_data":[{"full_name":"bentoml/BentoML","description":"The most flexible way to serve AI/ML models in production - Build Model Inference Service, LLM APIs, Inference Graph/Pipelines, Compound AI systems, Multi-Modal, RAG as a Service, and more!","topics":["model-serving","model-deployment","model-management","ml-platform","ai","machine-learning","mlops","bentoml","kubernetes","deep-learning"],"created_at":"2019-04-02T01:39:27Z","pushed_at":"2024-04-30T22:54:27Z","stargazers_count":6558,"language":"Python"}],"frecuent_topics":{"model-serving":1,"model-deployment":1,"model-management":1,"ml-platform":1,"ai":1}}